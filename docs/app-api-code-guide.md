# üìÇ API application code guide

The [API application](./app-api.md) will have the following top-level directories:

- `<package_name>/` - the directory with the application's source code. For the monorepo project structure (i.e., when Web UI is enabled) this directory will be named `api`. For the API-only project structure, this directory will have the same name as the project but in the snake case (i.e., if the project name is `example-chatbot` then the directory will be named `example_chatbot`).
- `migrations/` - the directory with the PostgreSQL database migrations generated by [Alembic](https://alembic.sqlalchemy.org/).
- `run/` - the directory containing the CLI commands for running the application locally and other scripts described above.

## üíæ Application source code

This section describes the structure of the `<package_name>/` directory containing the application's source code.

- `common` - the directory that contains common utilities, classes, and helper functions that are used by different application modules. This common functionality does not contain any application or business logic. In fact, these are the utilities that can be common for many other projects. For example:
  - `common/datetime.py` - contains the helper functions for working with dates and times.
  - `common/environ.py` - contains the helper functions for working with environment variables.
  - `common/sql.py` - contains the helper functions that mainly wrap the SQLAchemy/SQLModel's functions, making them type-safe, and adding some additional logic.
  - ... and other modules.

- `app` - the directory with the application's main module. It contains the application setup code, settings parsed from environment variables, database configurations, logging, common exception classes, generic request/response schemas, etc. Most probably you will want to modify at least the OpenAPI description of your project. For doing that go to the `app/app.py` file and modify the respective attributes passed to the `FastAPI` constructor.

### üé≥ Models and endpoints

- `models/` is the directory with the application's SQLModel/Pydantic models that represent the resources operated by the API:
  - `models/generic.py` - the generic Pydantic models that are used by other models that are persisted in the database.
  - `models/chat.py` - the Pydantic model for the `Chat` resource.
  - `models/message.py` - the Pydantic model for the `Message` resource.
  - `models/user.py` - the Pydantic model for the `User` resource.

  Feel free to adjust the existing models or add your own models here. The persisted models are usually inherited from the `GenericResource` declared in the `models/generic.py` module that defines the common fields for all the persisted resources like `id`, `created_at`, `modified_at`, `deleted_at`.

- `crud/` is the directory with the application's CRUD (Create, Read, Update, Delete) services. Each CRUD service is associated with a specific resource. At the moment there are the following CRUD services:
  - `crud/chat_crud.py` - the CRUD service for the `Chat` resource.
  - `crud/message_crud.py` - the CRUD service for the `Message` resource.

  All CRUD services are subclasses of the `AbstractCRUD` class. Feel free to adjust the existing CRUD services or add your own CRUD services here. For example, if you add a new persistent resource (e.g., `models/channel.py`, representing a conversation of multiple actors), then you will want to add a new CRUD service for it (e.g., `crud/channel_crud.py`).

- `routers/v1/` is the directory containing application routers for V1 API. A router is a group of related API endpoints, a.k.a., routes. Usually, each router is associated with a specific resource (model). The router package may contain the following modules:
  1. `<router_name>/models.py` - the module with the additional Pydantic models needed to represent the request and response data for the router's endpoints.
  2. `<router_name>/query.py` - the module containing the definitions of the query parameters for the router's endpoints.
  3. `<router_name>/router.py` - (required) the module with the actual router's endpoints.

  Feel free to adjust the existing routers or add your own routers following the examples and structure described above.

### üîê Authentication

`auth/` is the directory with the authentication-related code. At the moment it contains:

- `AbstractAuthenticator` class that is used as a base class for all the authenticators
- `LocalAuthenticator` class that is used for local authentication (i.e. authentication using the `./users.yaml` file) for development purposes.

Feel free to add your own authenticators here. Each authenticator class is associated with a specific `AUTH_PROVIDER` environment variable value. For example, the `LocalAuthenticator` class is associated with the `local` value of the `AUTH_PROVIDER` environment variable. If you create your own authenticator class, remember to add the mapping between the `AUTH_PROVIDER` value and your authenticator class in the `auth/__init__.py` file (see `get_authenticator` function).

When a custom authenticator is added, you will usually need to implement 3 methods:

- `authenticate(self, username: str, password: str)` - Authenticate user by username and password
- `verify_token(self, token: str)` - Verify the access token and return the user ID if the token is valid or `None` otherwise.
- `get_user(self, user_id: str)` - Get the user by ID.

Usually, when the API endpoint receives the client request with the access token, it calls the `verify_token` method of the authenticator to verify the token and get the user ID. There is also the API endpoint `/users/login` (see the file `routers/v1/users`) that uses the `authenticate` method. However, when a third-party identity provider is integrated on the client side - for example, the user authentication process is delegated to Firebase Auth, or Auth0, without ever passing user credentials to our backend API - then the `/users/login` endpoint is not used by the client application. Although, this endpoint may be still useful for API testing purposes, for example, if you want to receive the access token without interacting with the client application - in this case, when `/users/login` endpoint is requested, the backend should perform user authentication against the identity provider and return the access token (see the implementation of the `LocalAuthenticator` class for an example).

### üß† Generative AI

`ai/` is the directory with the code related to the AI-powered features of the application. At the moment it contains:

- `ai/llms/` - the package that defines the `LLMProvider`s, the abstraction layer for the LLMs provided by different vendors. At the moment there are 3 LLM providers supported:
  - `OpenAILLMProvider` - Azure OpenAI LLM provider
  - `VertexAILLMProvider` - Google VertexAI LLM provider
  - `BedrockLLMProvider` - Amazon Bedrock LLM provider

All the LLM providers are subclasses of the `AbstractLLMProvider` class. The current LLM provider is selected based on the `LLM_PROVIDER` environment variable value. For example, the `OpenAILLMProvider` class is associated with the `openai` value of the `LLM_PROVIDER` environment variable. See more configuration details in the `.env` file.

- `ai/assistants/` - the package that defines the different AI-powered assistants for the chatbot. The assistant is basically a wrapper around a Langchain chain, with a configured prompt, LLM, and additional logic processing the user inputs as well as the LLM outputs. At the moment there are the following assistants available out of the box, which you may use as an example for your own assistant, or modify them adapting their logic to your needs:
  - `ConversationAssistantBuffered` - the assistant that allows the user to have a conversation with the chatbot. It processes the incoming user message, takes into account the previous conversation history, and generates the AI response. The response is generated using the "buffered" approach, i.e., the assistant waits until the LLM generates the complete response and only then returns the result.
  - `ConversationAssistantStreamed` - the assistant is similar to the `ConversationAssistantBuffered`, but it uses the "streamed" approach, i.e. it starts returning the response gradually as soon as the LLM generates the first token.
  - `SubjectLineAssistant` - the assistant that generates the subject line based on the conversation history between the user and the AI assistant.
  - `ConversationRetrievalAssistantBuffered` - the assistant that allows the user to have a conversation with the chatbot where the user message is enriched with additional context information retrieved from the document index - a.k.a., Retrieval Augmented Generation (RAG). The response is generated using the "buffered" approach, i.e. the assistant waits until the LLM generates the complete response and only then returns the result.
  - `ConversationRetrievalAssistantStreamed` - the assistant similar to the `ConversationRetrievalAssistantBuffered`, but it uses the "streamed" approach, i.e., it starts returning the response gradually as soon as the LLM generates the first token.

  All the assistants are subclasses of the `AbstractAssistant` or `AbstractBasicAssistant` class. Feel free to adjust the existing assistants or add your own assistants. Usually, you will need to inherit from the `AbstractBasicAssistant` class, which provides the basic functionality for the assistant - in that case, you will need to override the following methods:

  - `_get_prompt_template` - returns the prompt that will be used by the assistant to generate the response.
  - `_get_stop_sequence` (optional) - returns the sequence of tokens that will be used by the assistant to determine when to stop the response generation.
  - `_get_llm` - returns the LLM provider that will be used by the assistant to generate the response.

  By default, the `AbstractBasicAssistant` class uses the `LLMChain`. However, if it is not enough for your needs, you can override the `_get_chain` method and return any other Langchain chain that the assistant should use.

  After you have overridden the methods mentioned above, you can add a `generate` method to your assistant, which should call either `self._run_buffered(...)` or `self._run_streamed(...)`  methods depending on whether you want to use the "buffered" or "streamed" approach. Keep in mind that if you use the "streamed" approach, then the LLM returned by the `_get_llm` method should support the streaming mode. For more details see the example implementations of `ConversationAssistantBuffered`, `ConversationAssistantStreamed`, and `SubjectLineAssistant` classes.

  Alternatively, you can inherit your assistant class from `AbstractAssistant` that provides `self._run_chain_buffered(...)` or `self._run_chain_streamed(...)` methods, where you would need to explicitly pass an instance of the Langchain chain to use for the response generation. This approach is useful when the chain to run needs to be dynamically configured based on the assistant input. For example, the `ConversationRetrievalAssistantBuffered` and `ConversationRetrievalAssistantStreamed` classes use this approach.

  For the assistant that uses the "streamed" approach, you may also want to override the `_get_response_chain` method - if the assistant uses a complex chain that consists of multiple sub-chains then this method should return the "final" chain whose output will be returned to the user. For example, the `ConversationRetrievalAssistantStreamed` uses `langchain.chains.ConversationalRetrievalChain` that consists of two chains under the hood - the first chain `condense_question_chain` that condenses the question and the previous conversation history into a single prompt, and the second chain `combine_docs_chain` that combines the condensed question with the retrieved documents and generates the response. In this case, the `_get_response_chain` method should return `combine_docs_chain` chain because it is the "final" chain whose output will be returned to the user.
