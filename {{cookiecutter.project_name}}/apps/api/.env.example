APP_ENV=local
APP_LOCALHOST_PORT=3000

# APP_WEB_DOMAINS: comma-separated list of domains for which the app is allowed to serve requests (i.e., CORS)
APP_WEB_DOMAINS=
# APP_WEB_LOCALHOST_PORTS: comma-separated list of localhost ports for which the app is allowed to serve requests (i.e., CORS)
APP_WEB_LOCALHOST_PORTS=3030

# AUTH_PROVIDER: either a single provider name (for example, "local" or "keycloak")
#                or composite list of providers (for example, "composite:keycloak,local")
#                if composite, the application will try to authenticate the user with each provider in the list in the order they are specified
#                make sure there are respective provider implementations in the ./api/auth package
AUTH_PROVIDER={{ cookiecutter.auth }}

{%- if cookiecutter.auth == "local" %}

AUTH_LOCAL_USERS_PATH=./users.yaml
AUTH_LOCAL_JWT_SECRET=11111111111111111111111111111111

{%- elif cookiecutter.auth == "keycloak" %}

AUTH_KEYCLOAK_REALM_NAME=app
AUTH_KEYCLOAK_SERVER_URL=http://localhost:8000
AUTH_KEYCLOAK_CLIENT_ID=api-server
AUTH_KEYCLOAK_CLIENT_SECRET=11111111111111111111111111111111

{%- endif %}

# TERMS_ADMIN_USERNAMES: comma-separated list of usernames that are allowed to update the terms of service
TERMS_ADMIN_USERNAMES=example@softserveinc.com

DB_HOST=localhost
DB_PORT=5432
DB_USERNAME=postgres
DB_PASSWORD=postgres
DB_NAME={{ cookiecutter.database_name }}
DB_ECHO=False
DB_SSL_MODE=prefer

# LLM_VERBOSE: whethere or not to print debug information about LLM requests
LLM_VERBOSE=False
# LLM_PROVIDER: possible values - openai, bedrock or vertexai
LLM_PROVIDER={{ cookiecutter.llm_provider }}
# LLM_CHAT_MAX_TOKENS: maximum number of tokens for chat LLM, depends on the configured LLM model
LLM_CHAT_MAX_TOKENS=4096
# LLM_TEXT_MAX_TOKENS: maximum number of tokens for text LLM, depends on the configured LLM model
LLM_TEXT_MAX_TOKENS=4096

# Azure OpenAI credentials are needed only if LLM_PROVIDER=openai
AZURE_OPENAI_API_KEY=11111111111111111111111111111111
AZURE_OPENAI_API_VERSION=2023-05-15
AZURE_OPENAI_ENDPOINT=https://FIXME-openai-resource-name.openai.azure.com/
AZURE_OPENAI_REQUEST_TIMEOUT=60
# AZURE_OPENAI_CHAT_MODEL: model for chat convesation, possible values - gpt-4, gpt-3.5-turbo
AZURE_OPENAI_CHAT_MODEL=gpt-3.5-turbo
AZURE_OPENAI_CHAT_DEPLOYMENT=FIXME-gpt-deployment-name
# AZURE_OPENAI_TEXT_MODEL: model for text processing (e.g., summarization), possible values - gpt-4, gpt-3.5-turbo
AZURE_OPENAI_TEXT_MODEL=gpt-3.5-turbo
AZURE_OPENAI_TEXT_DEPLOYMENT=FIXME-gpt-deployment-name
# AZURE_OPENAI_EMBEDDING_MODEL: model for generating vector embeddings for text, possible values - text-embedding-ada-002
AZURE_OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=FIXME-embedding-deployment-name

# AWS credentials are needed only if LLM_PROVIDER=bedrock
# AWS_PROFILE: if not specified the default credential profile or, if on an EC2 instance credentials from IMDS will be used.
AWS_PROFILE=FIXME-local-aws-profile-name
# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY: usually you may need these only if you deploy the app to environments other than AWS (e.g., GCP, Azure) while using AWS Bedrock as LLM provider.
# Otherwise, for local development, you will want to use AWS_PROFILE instead and for the app deployed to AWS, you will want to use IAM roles assigned to the ECS task.
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
# AWS_DEFAULT_REGION: if not specified then falls back to region specified in ~/.aws/config.
AWS_DEFAULT_REGION=us-east-1
# AWS_BEDROCK_CHAT_MODEL_ID: model for chat convesation, possible values - anthropic.claude-3-sonnet-20240229-v1:0, anthropic.claude-v2:1, meta.llama2-13b-chat-v1 - see https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html
AWS_BEDROCK_CHAT_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# AWS_BEDROCK_TEXT_MODEL_ID: model for text processing (e.g., summarization), possible values - amazon.titan-text-express-v1, ai21.j2-ultra-v1, anthropic.claude-v2:1 - see https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html
AWS_BEDROCK_TEXT_MODEL_ID=anthropic.claude-v2:1
# AWS_BEDROCK_EMBEDDING_MODEL_ID: model for generating vector embeddings for text, possible values - amazon.titan-embed-text-v1, cohere.embed-english-v3, cohere.embed-multilingual-v3
AWS_BEDROCK_EMBEDDING_MODEL_ID=amazon.titan-embed-text-v1

# GCP credentials are needed only if LLM_PROVIDER=vertexai
# GCP_PROJECT: Google Cloud Project ID https://cloud.google.com/resource-manager/docs/creating-managing-projects
GCP_PROJECT=FIXME-project-id
# GCP_SERVICE_ACCOUNT_KEY_PATH: If not provided, credentials will be ascertained from the environment or use `gcloud auth application-default login` locally, https://cloud.google.com/vertex-ai/docs/workbench/reference/authentication
GCP_SERVICE_ACCOUNT_KEY_PATH=./gcp-service-account-key.json
# GCP_LOCATION: possible values - us-central1, us-east1, us-west1, europe-west4 - see https://cloud.google.com/vertex-ai/docs/general/locations
GCP_LOCATION=us-central1
# GCP_VERTEXAI_CHAT_MODEL_NAME: model for chat convesation, possible values - chat-bison, codechat-bison - see https://console.cloud.google.com/vertex-ai/model-garden
GCP_VERTEXAI_CHAT_MODEL_NAME=chat-bison
# GCP_VERTEXAI_TEXT_MODEL_NAME: model for text processing (e.g., summarization), possible values - text-bison - see https://console.cloud.google.com/vertex-ai/model-garden
GCP_VERTEXAI_TEXT_MODEL_NAME=text-bison
# GCP_VERTEXAI_EMBEDDING_MODEL_NAME: model for generating vector embeddings for text, possible values - textembedding-gecko, textembedding-gecko-multilingual
GCP_VERTEXAI_EMBEDDING_MODEL_NAME=textembedding-gecko
